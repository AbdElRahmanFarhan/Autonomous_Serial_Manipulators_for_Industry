#include"opencv2/opencv.hpp"
#include <librealsense2/rs.hpp> // Include RealSense Cross Platform API
#include "/home/ahmedshehata/Libraries/librealsense/wrappers/opencv/cv-helpers.hpp"
#include <iostream>
#include <librealsense2/rs.hpp> // Include RealSense Cross Platform API
#include <librealsense2/rsutil.h>
#include "/home/ahmedshehata/Libraries/librealsense/examples/example.hpp"
#include "/home/ahmedshehata/Libraries/librealsense/third-party/imgui/imgui.h"
#include "/home/ahmedshehata/Libraries/librealsense/third-party/imgui/imgui_impl_glfw.h"
#include "opencv2/tracking.hpp"
#include "opencv2/imgcodecs.hpp"
#include "opencv2/imgproc.hpp"
#include "opencv2/videoio.hpp"
#include <opencv2/highgui.hpp>
#include <opencv2/video.hpp>
#include <opencv2/video/background_segm.hpp>
#include "/home/ahmedshehata/Libraries/librealsense/wrappers/opencv/cv-helpers.hpp"
#include <sstream>
#include <iostream>
#include <fstream>
#include <algorithm>
#include <cstring>
#include <chrono>
#include <eigen3/Eigen/Dense>
#include <librealsense2/rs.hpp> // Include RealSense Cross Platform API
#include <librealsense2/rsutil.h>
// This example will require several standard data-structures and algorithms:
#define _USE_MATH_DEFINES
#include <math.h>
#include <queue>
#include <unordered_set>
#include <map>
#include <thread>
#include <atomic>
#include <mutex>
#include "ros/ros.h"
#include "track3d/ball_trajectory.h"
#include <unistd.h>
#include"tf2/LinearMath/Transform.h"
//#include "sensor_msgs/Image.h"
//#include "cv_bridge/cv_bridge.h"
//#include "image_transport/image_transport.h"
#include "track3d/img_stream.h"



using namespace std;
using namespace cv;
using namespace std::chrono;
using pixel = std::pair<int, int>;

Vec3f dist_3d(rs2::depth_frame , pixel );

int src_depth_fps = 30, src_rgb_fps = 30;
int width_rgb = 1280, height_rgb = 720;
int width_depth = 1280, height_depth = 720;
const char* image_window = "Source Image";
const char* depth_window = "depth_window";

pixel mouse(0,0);

int low_r =0 , low_G = 0, low_b = 3;
int high_r = 99, high_G = 68, high_b = 217;
int max_value = 255;
const String window_detection_name = "Thresholding";
const String trackbar_window = "Trackbar";
int ref_roi_flag=0;

//fg mask fg mask generated by MOG2 method
int align_acc_flag=0;
high_resolution_clock::time_point t_frame1;
high_resolution_clock::time_point t_frame2;
float get_depth_scale(rs2::device dev);
rs2_stream find_stream_to_align(const std::vector<rs2::stream_profile>& streams);

//Trackbar defs--------------------------------------------
static void on_low_H_thresh_trackbar(int, void *)
{
    low_r = min(high_r-1, low_r);
    setTrackbarPos("Low R", trackbar_window, low_r);
}
static void on_high_H_thresh_trackbar(int, void *)
{
    high_r = max(high_r, low_r+1);
    setTrackbarPos("High R", trackbar_window, high_r);
}
static void on_low_S_thresh_trackbar(int, void *)
{
    low_G = min(high_G-1, low_G);
    setTrackbarPos("Low G", trackbar_window, low_G);
}
static void on_high_S_thresh_trackbar(int, void *)
{
    high_G = max(high_G, low_G+1);
    setTrackbarPos("High G", trackbar_window, high_G);
}
static void on_low_V_thresh_trackbar(int, void *)
{
    low_b = min(high_b-1, low_b);
    if(low_b%2 == 0)low_b+=1;
    setTrackbarPos("Low B", trackbar_window, low_b);
}
static void on_high_V_thresh_trackbar(int, void *)
{
    high_b = max(high_b, low_b+1);
    setTrackbarPos("High B", trackbar_window, high_b);
}
//---------------------------------------------------------------

void MouseCallbackFunc(int event, int x, int y, int flags, void* userdata)
{
    if  ( event == EVENT_LBUTTONDOWN )
    {
        mouse.first =  x;
        mouse.second = y;
        cout<<"dosnaaaaaaaaaaa"<<endl;
    }
}

//--------------- corner detection demo variables
#define SSTR( x ) static_cast< std::ostringstream & >( \
( std::ostringstream() << std::dec << x ) ).str()

int main(int argc, char *argv[])try
{
  namedWindow(window_detection_name, WINDOW_AUTOSIZE);
  namedWindow(image_window, WINDOW_AUTOSIZE);
  namedWindow(trackbar_window,WINDOW_AUTOSIZE);
  createTrackbar("Low R", trackbar_window, &low_r, max_value, on_low_H_thresh_trackbar);
  createTrackbar("High R", trackbar_window, &high_r, max_value, on_high_H_thresh_trackbar);
  createTrackbar("Low G", trackbar_window, &low_G, max_value, on_low_S_thresh_trackbar);
  createTrackbar("High G", trackbar_window, &high_G, max_value, on_high_S_thresh_trackbar);
  createTrackbar("Low B", trackbar_window, &low_b, max_value, on_low_V_thresh_trackbar);
  createTrackbar("High B", trackbar_window, &high_b, max_value, on_high_V_thresh_trackbar);
  std::mutex mutex;
  std::map<int, int> counters;
  Rect2d r;
  pixel cp1;

  int current_point = 0;
  const int points_num = 13;
  double cam_points[3][points_num];
  bool color_calibration_flag = false, prediction_test_flag = false;
  rs2::colorizer c;
  setMouseCallback(image_window, MouseCallbackFunc, NULL);

  //ROS specifics:
  //----------------
  //Initializing ROS node with a name of demo_topic_publisher
  ros::init(argc, argv,"fetch_and_align");
  //Created a nodehandle object
  ros::NodeHandle node_obj;
  //    image_transport::ImageTransport it(node_obj);
  ros::Publisher img_stream_pub = node_obj.advertise<track3d::img_stream>("/img_stream", 10);

  //camera variables:
  //-------------------
  //    namedWindow( image_window, WINDOW_AUTOSIZE );
  //    namedWindow( depth_window, WINDOW_NORMAL);
  //    resizeWindow(depth_window, width_depth,height_depth);
  rs2::decimation_filter dec;
  rs2::spatial_filter spat;
  rs2::hole_filling_filter holes;
  rs2::temporal_filter temp;
  rs2::threshold_filter thresh_filter;

  // Define transformations from and to Disparity domain
  rs2::disparity_transform depth2disparity;
  rs2::disparity_transform disparity2depth(false);
  //processing options
  dec.set_option(RS2_OPTION_FILTER_MAGNITUDE, 3);
  temp.set_option(RS2_OPTION_FILTER_SMOOTH_ALPHA,0.2);
  temp.set_option(RS2_OPTION_FILTER_SMOOTH_DELTA,100);
  temp.set_option(RS2_OPTION_HOLES_FILL,7);
  //    spat.set_option(RS2_OPTION_HOLES_FILL, 5); // 5 = fill all the zero pixels
  holes.set_option(RS2_OPTION_HOLES_FILL, 2);
  //    thresh_filter.set_option(RS2_OPTION_MAX_DISTANCE, 3.0); // Lab
  thresh_filter.set_option(RS2_OPTION_MAX_DISTANCE, 1.4);

  //stream profile and starting
  rs2::pipeline pipe;
  rs2::config cfg;
  const unsigned int CAPACITY = 1; // allow max latency of 10 frames
  const unsigned int stream_CAPACITY = 1; // allow max latqency of 10 frames
  rs2::frame_queue stream_frames(stream_CAPACITY);
  // After initial post-processing, frames will flow into this queue:
  rs2::frame_queue postprocessed_frames(CAPACITY);
  //    rs2::frame_queue postprocessed_frames(std::numeric_limits<unsigned int>::max());
  //    rs2::frame_queue stream_frames(std::numeric_limits<unsigned int>::max());
  // Define frame callback
  // The callback is executed on a sensor thread and can be called simultaneously from multiple sensors
  // Therefore any modification to common memory should be done under lock
  auto callback = [&](const rs2::frame& frame)
  {
    std::lock_guard<std::mutex> lock(mutex);
    if (rs2::frameset fs = frame.as<rs2::frameset>())
    {
      //            // With callbacks, all synchronized stream will arrive in a single frameset
      //            for (const rs2::frame& f : fs)
      //                counters[f.get_profile().unique_id()]++;
//      fs.keep();
      stream_frames.enqueue(fs);
    }
    else
    {
      // Stream that bypass synchronization (such as IMU) will produce single frames
      counters[frame.get_profile().unique_id()]++;
    }
  };
  //cfg.enable_device_from_file();
  cfg.enable_stream(RS2_STREAM_DEPTH,width_depth, height_depth,RS2_FORMAT_Z16,src_depth_fps);// Enable default depth
  cfg.enable_stream(RS2_STREAM_COLOR,width_rgb, height_rgb,RS2_FORMAT_RGBA8,src_rgb_fps);
//  cfg.enable_stream(RS2_STREAM_INFRARED,1,width_depth, height_depth,RS2_FORMAT_Y8,src_depth_fps);

  auto profile = pipe.start(cfg,callback);
  //aligning of the color and depth streams
  rs2_stream align_to = find_stream_to_align(profile.get_streams());
  rs2::align align(align_to);

  //geting sensor info
  auto sensor = profile.get_device().first<rs2::depth_sensor>();
  auto depth_scale = get_depth_scale(profile.get_device());
  // Alive boolean will signal the worker threads to finish-up
  std::atomic_bool alive{ true };
  //frame capturing and alignment thread:
  //-----------------------------------------
  std::thread video_processing_thread([&]() {
    // In order to generate new composite frames, we have to wrap the processing
    // code in a lambda
    rs2::processing_block frame_processor(
          [&](rs2::frameset data, // Input frameset (from the pipeline)
          rs2::frame_source& source) // Frame pool that can allocate new frames
    {
      t_frame1= high_resolution_clock::now();
      data = data.apply_filter(align); //Here we align
      // Decimation will reduce the resultion of the depth image,
      // closing small holes and speeding-up the algorithm
      //            data = data.apply_filter(dec);
//      data = data.apply_filter(holes);
      // To make sure far-away objects are filtered proportionally
      // we try to switch to disparity domain
      data = data.apply_filter(depth2disparity);

      //            // Apply spatial filtering
      //            data = data.apply_filter(spat);

      // Apply temporal filtering
      data = data.apply_filter(temp);

      // If we are in disparity domain, switch back to depth
      data = data.apply_filter(disparity2depth);
      //            data = data.apply_filter(c);
//      data.keep();
      //            if(start_flag)data.keep();
      t_frame2 = high_resolution_clock::now();
      auto duration2 = duration_cast<milliseconds>(t_frame2-t_frame1).count();
      //            cout<<"Alignment time="<<duration2<<endl;
      source.frame_ready(data);
    });

    // Indicate that we want the results of frame_processor
    // to be pushed into postprocessed_frames queue
    frame_processor >> postprocessed_frames;
    while (alive)
    {
      std::this_thread::sleep_for(std::chrono::milliseconds(1));
      std::lock_guard<std::mutex> lock(mutex);
      // Fetch frames from the pipeline and send them for processing
      rs2::frameset fs;
      //            if (pipe.poll_for_frames(&fs)) frame_processor.invoke(fs);
      if(stream_frames.poll_for_frame(&fs))
      {
        //               cout<<"thread__in"<<endl;
        frame_processor.invoke(fs);
      }
    }
  });
  while (ros::ok())
  {
    static rs2::frameset current_frameset;
    if(postprocessed_frames.poll_for_frame(&current_frameset))
    {
//      auto infrared_frame = current_frameset.get_infrared_frame();
      auto color_frame = current_frameset.get_color_frame();
      auto depth_frame = current_frameset.get_depth_frame();
//      Mat infrared_img = frame_to_mat(infrared_frame).clone(), frame_threshold;
      Mat color_img = frame_to_mat(color_frame).clone(), frame_threshold;
      Mat depth_in_meters = depth_frame_to_meters(pipe,depth_frame).clone();
      depth_in_meters.convertTo(depth_in_meters, CV_8UC1);

//      medianBlur(infrared_img,infrared_img,low_b);
//      inRange(infrared_img,low_r,high_r,frame_threshold);
      inRange(color_img,Scalar(low_r,low_G,low_b),Scalar(high_r,high_G,high_b),frame_threshold);
//      threshold(frame_threshold,frame_threshold,low_r,255,CV_THRESH_BINARY_INV);
      depth_in_meters = frame_threshold.mul(depth_in_meters);

//      imshow(image_window,infrared_img);
      imshow(image_window,color_img);
      waitKey(1);
//      imshow("depth_window", depth_in_meters);
//      waitKey(1);
      imshow(window_detection_name,frame_threshold);
      waitKey(1);

      if(waitKey(20) != 's' && ref_roi_flag == 0)continue;
      if(ref_roi_flag==0)
      {
        r = selectROI(frame_threshold);
      }
      // Crop image
      vector<vector<Point> > contours;
      vector<Vec4i> hierarchy;
      Mat imgCrop = frame_threshold(r);
      findContours( imgCrop, contours, hierarchy, CV_RETR_TREE, CV_CHAIN_APPROX_SIMPLE, Point(0, 0) );
      Mat drawing = Mat::zeros( imgCrop.size(), CV_8UC1 );
      for( int i = 0; i< contours.size(); i++ )
      {
        drawContours( drawing, contours, i, Scalar(255,255,255), FILLED, 8, hierarchy, 0, Point() );
      }
      imshow( "Contours", drawing );
      Moments m = moments(drawing);
      cp1.first = m.m10/m.m00 + r.x;
      cp1.second = m.m01/m.m00 + +r.y;
//      circle(infrared_img,Point(cp1.first,cp1.second),5,Scalar(255),2);
      circle(color_img,Point(cp1.first,cp1.second),5,Scalar(255,255,255),2);
      circle(depth_in_meters,Point(cp1.first,cp1.second),5,Scalar(0),2);

//      imshow("depth_window", depth_in_meters);
//      waitKey(1);
//      imshow(image_window,infrared_img);
      imshow(image_window,color_img);
      waitKey(1);
      if(ref_roi_flag==0) waitKey(0);

      Vec3f point_dist = dist_3d(depth_frame, cp1);
      if(point_dist.val[2] >= -0.1 && point_dist.val[2] <= 0.1)
      {
        cout<<"Error::Depth is equal to Zero!!!!!!!!!!!!!!!!"<<endl;
        ref_roi_flag=1;
        continue;
      }
      if(ref_roi_flag == 1)
      {
        cout<<"Notice :: Depth Found and Stored"<<endl;
        ref_roi_flag=0;
      }
      cam_points[0][current_point] = point_dist.val[0];
      cam_points[1][current_point] = point_dist.val[1];
      cam_points[2][current_point] = point_dist.val[2];
      cout<<cam_points[0][current_point]<<endl<<cam_points[1][current_point]<<endl<<cam_points[2][current_point]<<endl;
      current_point++;
      if(current_point == points_num)
      {
        cout<<"[";
        for(int row =0;row<3;row++)
        {
          cout<<"[";
          for(int col =0;col<points_num;col++)
          {
            cout<<cam_points[row][col];
            if(col != points_num-1)cout<<",";
          }
          cout<<"]"<<endl;
        }
        cout<<"]"<<endl;
        break;
      }
    }
  }
  cvDestroyAllWindows();
  pipe.stop();
  alive = false;
  video_processing_thread.join();
  return EXIT_SUCCESS;
}
catch (const rs2::error & e)
{
    std::cerr << "RealSense error calling " << e.get_failed_function() << "(" << e.get_failed_args() << "):\n    " << e.what() << std::endl;
    return EXIT_FAILURE;
}
catch (const std::exception & e)
{
    std::cerr << e.what() << std::endl;
    return EXIT_FAILURE;
}

float get_depth_scale(rs2::device dev)
{
    // Go over the device's sensors
    for (rs2::sensor& sensor : dev.query_sensors())
    {
        // Check if the sensor if a depth sensor
        if (rs2::depth_sensor dpt = sensor.as<rs2::depth_sensor>())
        {
            return dpt.get_depth_scale();
        }
    }
    throw std::runtime_error("Device does not have a depth sensor");
}
rs2_stream find_stream_to_align(const std::vector<rs2::stream_profile>& streams)
{
    //Given a vector of streams, we try to find a depth stream and another stream to align depth with.
    //We prioritize color streams to make the view look better.
    //If color is not available, we take another stream that (other than depth)
    rs2_stream align_to = RS2_STREAM_ANY;
    bool depth_stream_found = false;
    bool color_stream_found = false;
    for (rs2::stream_profile sp : streams)
    {
        rs2_stream profile_stream = sp.stream_type();
        if (profile_stream != RS2_STREAM_DEPTH)
        {
            if (!color_stream_found)         //Prefer color
                align_to = profile_stream;

            if (profile_stream == RS2_STREAM_COLOR)
            {
                color_stream_found = true;
            }
        }
        else
        {
            depth_stream_found = true;
        }
    }

    if(!depth_stream_found)
        throw std::runtime_error("No Depth stream available");

    if (align_to == RS2_STREAM_ANY)
        throw std::runtime_error("No stream found to align with Depth");

    return align_to;
}
Vec3f dist_3d(rs2::depth_frame frame, pixel u)
{
    float upixel[2]; // From pixel
    float upoint[3]; // From point (in 3D)

    // Copy pixels into the arrays (to match rsutil signatures)
    upixel[0] = u.first;
    upixel[1] = u.second;

    auto udist = frame.get_distance(upixel[0], upixel[1]);
    rs2_intrinsics intr = frame.get_profile().as<rs2::video_stream_profile>().get_intrinsics(); // Calibration data
    rs2_deproject_pixel_to_point(upoint, &intr, upixel, udist);
    Vec3f upoint_vec;
    upoint_vec.val[0] = upoint[0];
    upoint_vec.val[1] = upoint[1];
    upoint_vec.val[2] = upoint[2];

    return upoint_vec;
}
