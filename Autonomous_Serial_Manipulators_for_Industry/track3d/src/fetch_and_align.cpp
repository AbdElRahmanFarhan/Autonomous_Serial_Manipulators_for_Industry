#include"opencv2/opencv.hpp"
#include <librealsense2/rs.hpp> // Include RealSense Cross Platform API
#include "/home/ahmedshehata/Libraries/librealsense/wrappers/opencv/cv-helpers.hpp"
#include <iostream>
#include <librealsense2/rs.hpp> // Include RealSense Cross Platform API
#include <librealsense2/rsutil.h>
#include "/home/ahmedshehata/Libraries/librealsense/examples/example.hpp"
#include "/home/ahmedshehata/Libraries/librealsense/third-party/imgui/imgui.h"
#include "/home/ahmedshehata/Libraries/librealsense/third-party/imgui/imgui_impl_glfw.h"
#include "opencv2/tracking.hpp"
#include "opencv2/imgcodecs.hpp"
#include "opencv2/imgproc.hpp"
#include "opencv2/videoio.hpp"
#include <opencv2/highgui.hpp>
#include <opencv2/video.hpp>
#include <opencv2/video/background_segm.hpp>
#include "/home/ahmedshehata/Libraries/librealsense/wrappers/opencv/cv-helpers.hpp"
#include <sstream>
#include <iostream>
#include <fstream>
#include <algorithm>
#include <cstring>
#include <chrono>
#include <eigen3/Eigen/Dense>
#include <librealsense2/rs.hpp> // Include RealSense Cross Platform API
#include <librealsense2/rsutil.h>
// This example will require several standard data-structures and algorithms:
#define _USE_MATH_DEFINES
#include <math.h>
#include <queue>
#include <unordered_set>
#include <map>
#include <thread>
#include <atomic>
#include <mutex>
#include "ros/ros.h"
#include "track3d/ball_trajectory.h"
#include <unistd.h>
#include"tf2/LinearMath/Transform.h"
//#include "sensor_msgs/Image.h"
//#include "cv_bridge/cv_bridge.h"
//#include "image_transport/image_transport.h"
#include "track3d/img_stream.h"



using namespace std;
using namespace cv;
using namespace std::chrono;

int src_depth_fps = 90, src_rgb_fps = 60;
int width_rgb = 848, height_rgb = 480;
int width_depth = 848, height_depth = 480;
const char* image_window = "Source Image";
const char* depth_window = "depth_window";

//fg mask fg mask generated by MOG2 method
int align_acc_flag=0;
high_resolution_clock::time_point t_frame1;
high_resolution_clock::time_point t_frame2;
float get_depth_scale(rs2::device dev);
rs2_stream find_stream_to_align(const std::vector<rs2::stream_profile>& streams);

//--------------- corner detection demo variables
#define SSTR( x ) static_cast< std::ostringstream & >( \
( std::ostringstream() << std::dec << x ) ).str()
int main(int argc, char *argv[])try
{
    std::mutex mutex,global_mutex;
    std::map<int, int> counters;
    int ref_assign_flag = 1;
    rs2::colorizer c;
    //ROS specifics:
    //----------------
    //Initializing ROS node with a name of demo_topic_publisher
    ros::init(argc, argv,"fetch_and_align");
    //Created a nodehandle object
    ros::NodeHandle node_obj;
//    image_transport::ImageTransport it(node_obj);
    ros::Publisher img_stream_pub = node_obj.advertise<track3d::img_stream>("/img_stream", 10);

    //camera variables:
    //-------------------
//    namedWindow( image_window, WINDOW_AUTOSIZE );
//    namedWindow( depth_window, WINDOW_NORMAL);
//    resizeWindow(depth_window, width_depth,height_depth);
    rs2::decimation_filter dec;
    rs2::spatial_filter spat;
    rs2::hole_filling_filter holes;
    rs2::temporal_filter temp;
    rs2::threshold_filter thresh_filter;

    // Define transformations from and to Disparity domain
    rs2::disparity_transform depth2disparity;
    rs2::disparity_transform disparity2depth(false);
    //processing options
    dec.set_option(RS2_OPTION_FILTER_MAGNITUDE, 2);
    temp.set_option(RS2_OPTION_FILTER_SMOOTH_ALPHA,0.2);
    temp.set_option(RS2_OPTION_FILTER_SMOOTH_DELTA,100);
    temp.set_option(RS2_OPTION_HOLES_FILL,1);
//    spat.set_option(RS2_OPTION_HOLES_FILL, 5); // 5 = fill all the zero pixels
    holes.set_option(RS2_OPTION_HOLES_FILL, 0);
//    thresh_filter.set_option(RS2_OPTION_MAX_DISTANCE, 3.0); // Lab
    thresh_filter.set_option(RS2_OPTION_MAX_DISTANCE, 2.2);

    //stream profile and starting
    rs2::pipeline pipe;
    rs2::config cfg;
    const unsigned int CAPACITY = 20; // allow max latency of 10 frames
    const unsigned int stream_CAPACITY = 20; // allow max latency of 10 frames
    rs2::frame_queue stream_frames(stream_CAPACITY);
    // After initial post-processing, frames will flow into this queue:
    rs2::frame_queue postprocessed_frames(CAPACITY);
//    rs2::frame_queue postprocessed_frames(std::numeric_limits<unsigned int>::max());
//    rs2::frame_queue stream_frames(std::numeric_limits<unsigned int>::max());
    // Define frame callback
    // The callback is executed on a sensor thread and can be called simultaneously from multiple sensors
    // Therefore any modification to common memory should be done under lock
    auto callback = [&](const rs2::frame& frame)
    {
        std::lock_guard<std::mutex> lock(mutex);
        if (rs2::frameset fs = frame.as<rs2::frameset>())
        {
//            // With callbacks, all synchronized stream will arrive in a single frameset
//            for (const rs2::frame& f : fs)
//                counters[f.get_profile().unique_id()]++;
            fs.keep();
            stream_frames.enqueue(fs);
//            cout<<"callback__in"<<endl;

        }
        else
        {
            // Stream that bypass synchronization (such as IMU) will produce single frames
            counters[frame.get_profile().unique_id()]++;
        }
    };
    //cfg.enable_device_from_file();
    cfg.enable_stream(RS2_STREAM_DEPTH,width_depth, height_depth,RS2_FORMAT_Z16,src_depth_fps);// Enable default depth
//    cfg.enable_stream(RS2_STREAM_COLOR,width_rgb, height_rgb,RS2_FORMAT_RGBA8,src_rgb_fps);
//    cfg.enable_stream(RS2_STREAM_INFRARED,1,width_depth, height_depth,RS2_FORMAT_Y8,src_depth_fps);
    auto profile = pipe.start(cfg,callback);
    //aligning of the color and depth streams
//    rs2_stream align_to = find_stream_to_align(profile.get_streams());
//    rs2::align align(align_to);

    //geting sensor info
    auto sensor = profile.get_device().first<rs2::depth_sensor>();
    auto depth_scale = get_depth_scale(profile.get_device());
    // Alive boolean will signal the worker threads to finish-up
    std::atomic_bool alive{ true };
    //frame capturing and alignment thread:
    //-----------------------------------------
    std::thread video_processing_thread([&]() {
        // In order to generate new composite frames, we have to wrap the processing
        // code in a lambda
        rs2::processing_block frame_processor(
                    [&](rs2::frameset data, // Input frameset (from the pipeline)
                    rs2::frame_source& source) // Frame pool that can allocate new frames
        {
            t_frame1= high_resolution_clock::now();
//            data = data.apply_filter(align); //Here we align
            // Decimation will reduce the resultion of the depth image,
            // closing small holes and speeding-up the algorithm
            data = data.apply_filter(dec);
//            data = data.apply_filter(holes);
            // To make sure far-away objects are filtered proportionally
            // we try to switch to disparity domain
//            data = data.apply_filter(depth2disparity);

//            // Apply spatial filtering
//            data = data.apply_filter(spat);

            // Apply temporal filtering
//            data = data.apply_filter(temp);

            // If we are in disparity domain, switch back to depth
//            data = data.apply_filter(disparity2depth);
            if(ref_assign_flag == 1)data = data.apply_filter(thresh_filter);
//            data = data.apply_filter(c);
            data.keep();
//            if(start_flag)data.keep();
            t_frame2 = high_resolution_clock::now();
            auto duration2 = duration_cast<milliseconds>(t_frame2-t_frame1).count();
//            cout<<"Alignment time="<<duration2<<endl;
            source.frame_ready(data);
        });

        // Indicate that we want the results of frame_processor
        // to be pushed into postprocessed_frames queue
        frame_processor >> postprocessed_frames;
        while (alive)
        {
            // Fetch frames from the pipeline and send them for processing
            rs2::frameset fs;
//            if (pipe.poll_for_frames(&fs)) frame_processor.invoke(fs);
           if(stream_frames.poll_for_frame(&fs))
           {
//               cout<<"thread__in"<<endl;
               frame_processor.invoke(fs);
           }
        }
    });
    while (ros::ok())
    {
        static rs2::frameset current_frameset;
        if(postprocessed_frames.poll_for_frame(&current_frameset))
        {
//          auto ir_frame = current_frameset.get_infrared_frame();
          auto depth_frame = current_frameset.get_depth_frame();
//          Mat ir_img = frame_to_mat(ir_frame).clone();
          //            Mat depth_img = depth_frame_to_meters(pipe, depth_frame).clone();
          Mat depth_img = frame_to_mat(depth_frame).clone();
//          std::vector<uchar> ir_arr;
          std::vector<uint16_t> depth_arr;
//          if (ir_img.isContinuous())
//          {
//            ir_arr.assign(ir_img.data, ir_img.data + ir_img.total());
//          }
//          else
//          {
//            for (int i = 0; i < ir_img.rows; ++i)
//            {
//              ir_arr.insert(ir_arr.end(), ir_img.ptr<uchar>(i), ir_img.ptr<uchar>(i)+ir_img.cols);
//            }
//          }
          if (depth_img.isContinuous())
          {
            depth_arr.assign((uint16_t*)depth_img.data, (uint16_t*)depth_img.data + depth_img.total());
          }
          else
          {
            for (int i = 0; i < depth_img.rows; ++i)
            {
              depth_arr.insert(depth_arr.end(), depth_img.ptr<uint16_t>(i), depth_img.ptr<uint16_t>(i)+depth_img.cols);
            }
          }
          //            std_msgs::Header header;
          //            header.stamp.fromSec(ir_frame.get_timestamp() * 0.001);
          //            header.frame_id = to_string(ir_frame.get_frame_number());
          track3d::img_stream img_stream_msg;
          rs2_intrinsics intr = depth_frame.get_profile().as<rs2::video_stream_profile>().get_intrinsics(); // Calibration data

          img_stream_msg.depth = depth_arr;
//          img_stream_msg.ir = ir_arr;
          img_stream_msg.time_stamp = depth_frame.get_timestamp();
          img_stream_msg.depth_scale = depth_scale;
          img_stream_msg.width = intr.width;
          img_stream_msg.height = intr.height;
          img_stream_msg.ppx = intr.ppx;
          img_stream_msg.ppy = intr.ppy;
          img_stream_msg.fx = intr.fx;
          img_stream_msg.fy = intr.fy;
          img_stream_msg.model = intr.model;
          img_stream_msg.coeffs.assign(intr.coeffs, intr.coeffs + 5);
          img_stream_pub.publish(img_stream_msg);
        }
    }

    alive = false;
    video_processing_thread.join();
    return EXIT_SUCCESS;
}
catch (const rs2::error & e)
{
    std::cerr << "RealSense error calling " << e.get_failed_function() << "(" << e.get_failed_args() << "):\n    " << e.what() << std::endl;
    return EXIT_FAILURE;
}
catch (const std::exception & e)
{
    std::cerr << e.what() << std::endl;
    return EXIT_FAILURE;
}

float get_depth_scale(rs2::device dev)
{
    // Go over the device's sensors
    for (rs2::sensor& sensor : dev.query_sensors())
    {
        // Check if the sensor if a depth sensor
        if (rs2::depth_sensor dpt = sensor.as<rs2::depth_sensor>())
        {
            return dpt.get_depth_scale();
        }
    }
    throw std::runtime_error("Device does not have a depth sensor");
}
rs2_stream find_stream_to_align(const std::vector<rs2::stream_profile>& streams)
{
    //Given a vector of streams, we try to find a depth stream and another stream to align depth with.
    //We prioritize color streams to make the view look better.
    //If color is not available, we take another stream that (other than depth)
    rs2_stream align_to = RS2_STREAM_ANY;
    bool depth_stream_found = false;
    bool color_stream_found = false;
    for (rs2::stream_profile sp : streams)
    {
        rs2_stream profile_stream = sp.stream_type();
        if (profile_stream != RS2_STREAM_DEPTH)
        {
            if (!color_stream_found)         //Prefer color
                align_to = profile_stream;

            if (profile_stream == RS2_STREAM_COLOR)
            {
                color_stream_found = true;
            }
        }
        else
        {
            depth_stream_found = true;
        }
    }

    if(!depth_stream_found)
        throw std::runtime_error("No Depth stream available");

    if (align_to == RS2_STREAM_ANY)
        throw std::runtime_error("No stream found to align with Depth");

    return align_to;
}
